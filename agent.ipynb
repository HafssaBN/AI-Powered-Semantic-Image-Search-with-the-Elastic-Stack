{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1fcaa23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's make sure we have the necessary libraries installed.\n",
    "# !pip install langgraph langchain langchain_groq groq sentence-transformers Pillow requests beautifulsoup4 serpapi numpy scikit-learn\n",
    "\n",
    "import os\n",
    "import logging\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Annotated, Sequence, TypedDict, List, Dict, Optional\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "\n",
    "# Langchain and LangGraph imports\n",
    "from langgraph.graph import StateGraph, END\n",
    "from langgraph.prebuilt import ToolNode\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_core.messages import BaseMessage, HumanMessage, SystemMessage, AIMessage, ToolMessage\n",
    "from langchain_core.tools import tool\n",
    "from langchain_groq import ChatGroq\n",
    "from groq import Groq\n",
    "\n",
    "# Image and ML imports\n",
    "from PIL import Image\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# --- Configuration & Setup ---\n",
    "\n",
    "# 1. Logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# 2. API Keys\n",
    "# IMPORTANT: For security, replace \"YOUR_...\" with your actual keys and consider using environment variables.\n",
    "os.environ[\"GROQ_API_KEY\"] = \"YOUR_GROQ_API_KEY\"\n",
    "SERPER_API_KEY = \"c4840737ba73d13a472ae78c5aa196d965a168c2\"\n",
    "SERPAPI_API_KEY = \"7d6e5e5f8bf83fc3be53bb89e3f2031188cdb0fec657bde729b9c2cdbbd660f5\"\n",
    "\n",
    "# 3. Directories\n",
    "BASE_OUTPUT_DIR = Path(\"osint_results\")\n",
    "LOCAL_IMAGE_DB_DIR = Path(\"static/images\")\n",
    "BASE_OUTPUT_DIR.mkdir(exist_ok=True)\n",
    "LOCAL_IMAGE_DB_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "\n",
    "# --- LLM and Model Initialization (Lazy Loading) ---\n",
    "\n",
    "llm = None\n",
    "clip_model = None\n",
    "\n",
    "def get_llm():\n",
    "    \"\"\"Initializes and returns the LLM, checking for API key.\"\"\"\n",
    "    global llm\n",
    "    if llm is None:\n",
    "        if not os.environ.get(\"GROQ_API_KEY\") or \"YOUR_\" in os.environ.get(\"GROQ_API_KEY\"):\n",
    "            raise ValueError(\"GROQ_API_KEY is not set. Please add your key to the script.\")\n",
    "        try:\n",
    "            llm = ChatGroq(model=\"gemma2-9b-it\", api_key=os.environ[\"GROQ_API_KEY\"], temperature=0.1)\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize LLM: {e}\")\n",
    "            raise\n",
    "    return llm\n",
    "\n",
    "def get_clip_model():\n",
    "    \"\"\"Loads the CLIP model only when needed.\"\"\"\n",
    "    global clip_model\n",
    "    if clip_model is None:\n",
    "        logger.info(\"Loading CLIP model for image similarity...\")\n",
    "        clip_model = SentenceTransformer(\"sentence-transformers/clip-ViT-B-32\")\n",
    "        logger.info(\"CLIP model loaded.\")\n",
    "    return clip_model\n",
    "\n",
    "\n",
    "# --- Agent State Definition ---\n",
    "\n",
    "class ListingAgentState(TypedDict):\n",
    "    messages: Annotated[Sequence[BaseMessage], add_messages]\n",
    "    listing_name: str\n",
    "    search_urls: List[str]\n",
    "    extracted_data: Dict[str, Dict]\n",
    "    similarity_report: Dict[str, List[Dict]]\n",
    "    final_report: str\n",
    "    completed_tasks: List[str]\n",
    "\n",
    "# --- Tool Definitions ---\n",
    "\n",
    "@tool\n",
    "def listing_url_search_tool(listing_name: str) -> str:\n",
    "    \"\"\"\n",
    "    Performs a search to find URLs related to an Airbnb listing.\n",
    "    Returns a JSON string of the search results.\n",
    "    \"\"\"\n",
    "    print(f\"üîé URL Search Tool - Searching for: {listing_name}\")\n",
    "    url = \"https://google.serper.dev/search\"\n",
    "    payload = json.dumps({\"q\": listing_name, \"location\": \"Morocco\", \"gl\": \"ma\", \"num\": 20})\n",
    "    headers = {'X-API-KEY': SERPER_API_KEY, 'Content-Type': 'application/json'}\n",
    "    try:\n",
    "        response = requests.request(\"POST\", url, headers=headers, data=payload, timeout=20)\n",
    "        response.raise_for_status()\n",
    "        results = response.json()\n",
    "        urls = [item['link'] for item in results.get('organic', []) if any(site in item['link'] for site in ['airbnb.com', 'booking.com', 'instagram.com', 'agoda.com', 'trivago'])]\n",
    "        print(f\"‚úÖ Found {len(urls)} relevant URLs.\")\n",
    "        return json.dumps(urls)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"URL search failed: {e}\")\n",
    "        return f\"Error: Could not perform search. {e}\"\n",
    "\n",
    "@tool\n",
    "def url_content_extractor_tool(url_to_scrape: str) -> str:\n",
    "    \"\"\"\n",
    "    Browses a single URL, extracts all images, saves them to a unique folder,\n",
    "    and extracts the host's name if possible.\n",
    "    \"\"\"\n",
    "    print(f\"üñºÔ∏è Content Extractor Tool - Scraping: {url_to_scrape}\")\n",
    "    try:\n",
    "        domain = urlparse(url_to_scrape).netloc.replace('.', '_')\n",
    "        output_dir = BASE_OUTPUT_DIR / domain\n",
    "        output_dir.mkdir(exist_ok=True)\n",
    "\n",
    "        headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'}\n",
    "        response = requests.get(url_to_scrape, headers=headers, timeout=30)\n",
    "        response.raise_for_status()\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "        image_urls = [img['src'] for img in soup.find_all('img') if img.get('src')]\n",
    "        saved_image_paths = []\n",
    "        for i, img_url in enumerate(image_urls):\n",
    "            if not img_url.startswith(('http:', 'https:')):\n",
    "                img_url = f\"{urlparse(url_to_scrape).scheme}://{urlparse(url_to_scrape).netloc}{img_url}\"\n",
    "            try:\n",
    "                img_response = requests.get(img_url, stream=True, timeout=15)\n",
    "                if img_response.status_code == 200:\n",
    "                    img_name = Path(urlparse(img_url).path).name\n",
    "                    if not img_name or len(img_name) > 100: img_name = f\"image_{i}.jpg\"\n",
    "                    save_path = output_dir / img_name\n",
    "                    with open(save_path, 'wb') as f:\n",
    "                        for chunk in img_response.iter_content(1024): f.write(chunk)\n",
    "                    saved_image_paths.append(str(save_path))\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Could not download image {img_url}: {e}\")\n",
    "\n",
    "        host_name = \"Not found\"\n",
    "        text_content = soup.get_text(\" \", strip=True)\n",
    "        match = re.search(r\"(?i)(hosted by|host:|g√©r√© par|owner:|propri√©taire:)\\s*([A-Z][a-zA-Z\\s]+)\", text_content)\n",
    "        if match: host_name = match.group(2).strip()\n",
    "\n",
    "        result = {\"source_url\": url_to_scrape, \"saved_images\": saved_image_paths, \"found_host\": host_name}\n",
    "        print(f\"‚úÖ Extracted {len(saved_image_paths)} images and host '{host_name}' from {url_to_scrape}\")\n",
    "        return json.dumps(result)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to scrape {url_to_scrape}: {e}\")\n",
    "        return f\"Error: Could not scrape URL. {e}\"\n",
    "\n",
    "@tool\n",
    "def image_similarity_tool(new_image_paths_json: str) -> str:\n",
    "    \"\"\"\n",
    "    Compares new images against a local DB to find visual similarities using a CLIP model.\n",
    "    \"\"\"\n",
    "    print(f\"‚ú® Image Similarity Tool - Comparing images...\")\n",
    "    new_image_paths = json.loads(new_image_paths_json)\n",
    "    if not new_image_paths: return \"No new images provided to compare.\"\n",
    "    try:\n",
    "        model = get_clip_model()\n",
    "        db_image_paths = [p for p in LOCAL_IMAGE_DB_DIR.glob('*') if p.suffix.lower() in ['.png', '.jpg', '.jpeg']]\n",
    "        if not db_image_paths: return \"No images found in the local database to compare against.\"\n",
    "        db_embeddings = model.encode([Image.open(p) for p in db_image_paths])\n",
    "        report = {}\n",
    "        for new_img_path_str in new_image_paths:\n",
    "            new_img_path = Path(new_img_path_str)\n",
    "            if not new_img_path.exists(): continue\n",
    "            new_embedding = model.encode(Image.open(new_img_path)).reshape(1, -1)\n",
    "            scores = cosine_similarity(new_embedding, db_embeddings)[0]\n",
    "            top_indices = scores.argsort()[-3:][::-1]\n",
    "            matches = [{\"local_image\": str(db_image_paths[i]), \"score\": float(scores[i])} for i in top_indices if scores[i] > 0.85]\n",
    "            if matches: report[str(new_img_path)] = matches\n",
    "        print(f\"‚úÖ Similarity check complete. Found {len(report)} potential matches.\")\n",
    "        return json.dumps(report, indent=2)\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Image similarity check failed: {e}\", exc_info=True)\n",
    "        return f\"Error during similarity check: {e}\"\n",
    "\n",
    "@tool\n",
    "def final_report_generation_tool(listing_name: str, extracted_data_json: str, similarity_report_json: str) -> str:\n",
    "    \"\"\"Generates a final OSINT report from all gathered findings.\"\"\"\n",
    "    print(\"üìã Final Report Tool - Compiling all findings...\")\n",
    "    prompt = f\"\"\"You are an OSINT analyst. Compile a detailed intelligence report for the Airbnb listing: \"{listing_name}\".\n",
    "Structure the report clearly with the following sections. Be factual and only use the provided data.\n",
    "**Data Provided:**\n",
    "- Extracted Website Data: {extracted_data_json}\n",
    "- Image Similarity Analysis: {similarity_report_json}\n",
    "**Report Structure:**\n",
    "1. **Executive Summary:** Brief overview of key findings. Was the listing verified? Was a host identified? Are there visual matches to known images?\n",
    "2. **Online Presence Analysis:** List all URLs where the listing was found.\n",
    "3. **Visual Verification (Image Matching):** Detail the findings. For each new image that matched a local one, present the evidence: `New Image -> Local DB Image (Similarity Score)`. Conclude if the images from the web strongly match the local database.\n",
    "4. **Host Identification:** State the name of the host found and from which website.\n",
    "5. **Conclusion:** Summarize the investigation and provide a confidence level on whether the online listings are for the same entity as the one in the local database.\"\"\"\n",
    "    try:\n",
    "        response = get_llm().invoke([SystemMessage(content=prompt)])\n",
    "        print(\"‚úÖ Final Report Generated.\")\n",
    "        return response.content\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Report generation failed: {e}\")\n",
    "        return f\"Error generating report: {e}\"\n",
    "\n",
    "# --- Graph Nodes ---\n",
    "\n",
    "def coordinator_node(state: ListingAgentState) -> dict:\n",
    "    \"\"\"Decides which tool to call next.\"\"\"\n",
    "    print(\"\\nüéØ Coordinator Agent - Deciding next step...\")\n",
    "    completed = state.get(\"completed_tasks\", [])\n",
    "    if \"url_search\" not in completed:\n",
    "        print(\"üìã Task: Search for listing URLs.\")\n",
    "        tool_call = get_llm().bind_tools([listing_url_search_tool]).invoke(f\"Find all relevant online URLs for the listing: {state['listing_name']}\")\n",
    "        return {\"messages\": [tool_call]}\n",
    "    if \"content_extraction\" not in completed: return {}\n",
    "    if \"similarity_check\" not in completed:\n",
    "        print(\"üìã Task: Perform image similarity check.\")\n",
    "        all_new_images = [img for data in state.get(\"extracted_data\", {}).values() for img in data.get(\"saved_images\", [])]\n",
    "        tool_call = AIMessage(\"\", tool_calls=[{\"name\": \"image_similarity_tool\", \"args\": {\"new_image_paths_json\": json.dumps(all_new_images)}, \"id\": f\"call_similarity_{time.time()}\"}])\n",
    "        return {\"messages\": [tool_call]}\n",
    "    if \"final_report\" not in completed:\n",
    "        print(\"üìã Task: Generate final report.\")\n",
    "        tool_call = AIMessage(\"\", tool_calls=[{\"name\": \"final_report_generation_tool\", \"args\": {\"listing_name\": state['listing_name'], \"extracted_data_json\": json.dumps(state.get(\"extracted_data\", {})), \"similarity_report_json\": json.dumps(state.get(\"similarity_report\", {}))}, \"id\": f\"call_report_{time.time()}\"}])\n",
    "        return {\"messages\": [tool_call]}\n",
    "    return {}\n",
    "\n",
    "def tool_execution_node(state: ListingAgentState) -> dict:\n",
    "    \"\"\"Executes tools and updates the state.\"\"\"\n",
    "    print(\"üîß Tool Execution Node - Running tools...\")\n",
    "    if \"url_search\" in state[\"completed_tasks\"] and \"content_extraction\" not in state[\"completed_tasks\"]:\n",
    "        urls_to_scrape = state.get(\"search_urls\", [])\n",
    "        if not urls_to_scrape:\n",
    "            print(\"‚ö†Ô∏è No URLs to scrape. Skipping content extraction.\")\n",
    "            return {\"completed_tasks\": state[\"completed_tasks\"] + [\"content_extraction\"]}\n",
    "        extracted_data = state.get(\"extracted_data\", {})\n",
    "        for url in urls_to_scrape:\n",
    "            result_json = url_content_extractor_tool.invoke({\"url_to_scrape\": url})\n",
    "            try: extracted_data[url] = json.loads(result_json)\n",
    "            except json.JSONDecodeError: extracted_data[url] = {\"error\": result_json}\n",
    "        return {\"extracted_data\": extracted_data, \"completed_tasks\": state[\"completed_tasks\"] + [\"content_extraction\"]}\n",
    "    \n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if not hasattr(last_message, \"tool_calls\") or not last_message.tool_calls: return {}\n",
    "    \n",
    "    tool_call = last_message.tool_calls[0]\n",
    "    all_tools = {\"listing_url_search_tool\": listing_url_search_tool, \"image_similarity_tool\": image_similarity_tool, \"final_report_generation_tool\": final_report_generation_tool}\n",
    "    if tool_call['name'] not in all_tools: return {\"messages\": [ToolMessage(content=f\"Error: Tool '{tool_call['name']}' not found.\", tool_call_id=tool_call['id'])]}\n",
    "    \n",
    "    try:\n",
    "        result = all_tools[tool_call['name']].invoke(tool_call['args'])\n",
    "        tool_message = ToolMessage(content=str(result), tool_call_id=tool_call['id'])\n",
    "        updated_state, completed_tasks = {\"messages\": [tool_message]}, state.get(\"completed_tasks\", []).copy()\n",
    "        if tool_call['name'] == \"listing_url_search_tool\":\n",
    "            updated_state[\"search_urls\"], completed_tasks.append(\"url_search\") = json.loads(result), print(\"‚úÖ url_search completed.\")\n",
    "        elif tool_call['name'] == \"image_similarity_tool\":\n",
    "            updated_state[\"similarity_report\"], completed_tasks.append(\"similarity_check\") = json.loads(result), print(\"‚úÖ similarity_check completed.\")\n",
    "        elif tool_call['name'] == \"final_report_generation_tool\":\n",
    "            updated_state[\"final_report\"], completed_tasks.append(\"final_report\") = result, print(\"üéâ Final report generated!\")\n",
    "        updated_state[\"completed_tasks\"] = completed_tasks\n",
    "        return updated_state\n",
    "    except Exception as e:\n",
    "        error_msg = f\"Error executing tool {tool_call['name']}: {e}\"\n",
    "        logger.error(error_msg, exc_info=True)\n",
    "        return {\"messages\": [ToolMessage(content=error_msg, tool_call_id=tool_call['id'])]}\n",
    "\n",
    "# --- Graph Definition & Execution ---\n",
    "def should_continue(state: ListingAgentState) -> str:\n",
    "    if \"final_report\" in state.get(\"completed_tasks\", []): return \"end\"\n",
    "    last_message = state[\"messages\"][-1]\n",
    "    if hasattr(last_message, 'tool_calls') and last_message.tool_calls: return \"execute_tools\"\n",
    "    return \"coordinate\"\n",
    "\n",
    "workflow = StateGraph(ListingAgentState)\n",
    "workflow.add_node(\"coordinator\", coordinator_node)\n",
    "workflow.add_node(\"tools\", tool_execution_node)\n",
    "workflow.set_entry_point(\"coordinator\")\n",
    "workflow.add_conditional_edges(\"coordinator\", should_continue, {\"execute_tools\": \"tools\", \"end\": END, \"coordinate\": \"coordinator\"})\n",
    "workflow.add_edge(\"tools\", \"coordinator\")\n",
    "app = workflow.compile()\n",
    "\n",
    "# --- Main Application Logic ---\n",
    "def run_listing_investigation(listing_name: str):\n",
    "    \"\"\"Starts and runs the entire investigation process.\"\"\"\n",
    "    print(f\"\\nüöÄ Launching OSINT investigation for: \\\"{listing_name}\\\" üöÄ\")\n",
    "    if not any(LOCAL_IMAGE_DB_DIR.iterdir()):\n",
    "        print(\"Local image DB is empty. Creating dummy images for demonstration.\")\n",
    "        Image.new('RGB', (100, 100), color = 'red').save(LOCAL_IMAGE_DB_DIR / 'known_image_1.jpg')\n",
    "        Image.new('RGB', (100, 100), color = 'green').save(LOCAL_IMAGE_DB_DIR / 'known_image_2.jpg')\n",
    "    initial_state = {\"messages\": [HumanMessage(content=f\"Start investigation for {listing_name}\")], \"listing_name\": listing_name, \"completed_tasks\": []}\n",
    "    try:\n",
    "        final_state = None\n",
    "        for state in app.stream(initial_state, {\"recursion_limit\": 15}): final_state = state\n",
    "        print(\"\\n\\n‚úÖ Investigation Complete! ‚úÖ\")\n",
    "        report = final_state.get(\"final_report\", \"No report was generated.\")\n",
    "        print(\"\\n\" + \"=\"*80 + \"\\nFINAL OSINT REPORT\\n\" + \"=\"*80 + f\"\\n{report}\")\n",
    "        report_filename = f\"{listing_name.replace(' ', '_')}_osint_report.md\"\n",
    "        with open(BASE_OUTPUT_DIR / report_filename, 'w', encoding='utf-8') as f: f.write(report)\n",
    "        print(f\"\\nüíæ Report saved to: {BASE_OUTPUT_DIR / report_filename}\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"An error occurred during the investigation: {e}\", exc_info=True)\n",
    "\n",
    "def main_chat_loop():\n",
    "    \"\"\"Main conversational loop to interact with the system.\"\"\"\n",
    "    print(\"üí¨ OSINT Chatbot is live!\")\n",
    "    print(\"You can ask me to 'run a report on Riad Green Vines' or 'investigate Riad Cherifa'. Type 'exit' to quit.\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            user_input = input(\"\\nüë§ You: \")\n",
    "            if user_input.strip().lower() in [\"exit\", \"quit\"]:\n",
    "                print(\"üëã Exiting. Goodbye!\")\n",
    "                break\n",
    "\n",
    "            decision_prompt = f\"\"\"You are a dispatch assistant. Your job is to determine if the user wants to run an investigation on a property.\n",
    "If they mention words like 'investigate', 'report on', 'deep dive', 'research', or 'find info on' followed by a property name, respond with the property name ONLY.\n",
    "Otherwise, respond with 'NONE'.\n",
    "\n",
    "User request: \"{user_input}\" \"\"\"\n",
    "            \n",
    "            response = get_llm().invoke([SystemMessage(content=decision_prompt)]).content.strip()\n",
    "            \n",
    "            if response.upper() != \"NONE\":\n",
    "                run_listing_investigation(response)\n",
    "                print(\"\\nüí¨ You can now continue chatting or request another investigation.\")\n",
    "            else:\n",
    "                # General chat response\n",
    "                chat_response = get_llm().invoke([HumanMessage(content=user_input)]).content\n",
    "                print(f\"ü§ñ {chat_response}\")\n",
    "\n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\nüëã Exiting. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"‚ùå An unexpected error occurred: {str(e)}\")\n",
    "            logger.error(\"Error in main chat loop\", exc_info=True)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        get_llm() # Pre-check for API key at startup\n",
    "        main_chat_loop()\n",
    "    except ValueError as e:\n",
    "        print(f\"üö® Configuration Error: {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"üö® A critical error occurred on startup: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
